{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06_pytorch_transfer_learning_exercises.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "57241e3ef94a42879becdbf83e300570": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53cdb27fecaf4e0fbfca1477b46846ca",
              "IPY_MODEL_32e4b2b439c94e1cbb3fb5227e6a0e8e",
              "IPY_MODEL_9ed5232a64dd415db8c482296c340efe"
            ],
            "layout": "IPY_MODEL_d1c55914b6a7415caffedfb0fe3ee017"
          }
        },
        "53cdb27fecaf4e0fbfca1477b46846ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45850c5b1c2843cf8ba0217f5a0c8e3d",
            "placeholder": "​",
            "style": "IPY_MODEL_1a10a63d287c4656972c407d8f251ab2",
            "value": "100%"
          }
        },
        "32e4b2b439c94e1cbb3fb5227e6a0e8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df354546b71490d8fc3890d49ed564a",
            "max": 5,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45eda459e5584bc599d35a655a814519",
            "value": 5
          }
        },
        "9ed5232a64dd415db8c482296c340efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1532d532ccc4df6991d37f6375a7ce8",
            "placeholder": "​",
            "style": "IPY_MODEL_0162e6980bb84270a640f9c7bc0e4b1f",
            "value": " 5/5 [00:22&lt;00:00,  3.95s/it]"
          }
        },
        "d1c55914b6a7415caffedfb0fe3ee017": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45850c5b1c2843cf8ba0217f5a0c8e3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a10a63d287c4656972c407d8f251ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7df354546b71490d8fc3890d49ed564a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45eda459e5584bc599d35a655a814519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1532d532ccc4df6991d37f6375a7ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0162e6980bb84270a640f9c7bc0e4b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danilovpy/learnpytorchioexe/blob/main/extras/exercises/06_pytorch_transfer_learning_exercises.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 06. PyTorch Transfer Learning Exercises\n",
        "\n",
        "Welcome to the 06. PyTorch Transfer Learning exercise template notebook.\n",
        "\n",
        "There are several questions in this notebook and it's your goal to answer them by writing Python and PyTorch code.\n",
        "\n",
        "> **Note:** There may be more than one solution to each of the exercises, don't worry too much about the *exact* right answer. Try to write some code that works first and then improve it if you can.\n",
        "\n",
        "## Resources and solutions\n",
        "\n",
        "* These exercises/solutions are based on [section 06. PyTorch Transfer Learning](https://www.learnpytorch.io/06_pytorch_transfer_learning/) of the Learn PyTorch for Deep Learning course by Zero to Mastery.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "Try to complete the code below *before* looking at these.\n",
        "\n",
        "* See a live [walkthrough of the solutions (errors and all) on YouTube](https://youtu.be/ueLolShyFqs).\n",
        "* See an example [solutions notebook for these exercises on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/06_pytorch_transfer_learning_exercise_solutions.ipynb)."
      ],
      "metadata": {
        "id": "zNqPNlYylluR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Make predictions on the entire test dataset and plot a confusion matrix for the results of our model compared to the truth labels.\n",
        "* **Note:** You will need to get the dataset and the trained model/retrain the model from notebook 06 to perform predictions.\n",
        "* Check out [03. PyTorch Computer Vision section 10](https://www.learnpytorch.io/03_pytorch_computer_vision/#10-making-a-confusion-matrix-for-further-prediction-evaluation) for ideas."
      ],
      "metadata": {
        "id": "nwmoMhW8IqSu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries/code\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms, datasets\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ],
      "metadata": {
        "id": "nqtAWBUJgaF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a56393-c9a9-4340-b8d4-dbcf96f89e51"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4393, done.\u001b[K\n",
            "remote: Counting objects: 100% (1534/1534), done.\u001b[K\n",
            "remote: Compressing objects: 100% (133/133), done.\u001b[K\n",
            "remote: Total 4393 (delta 1457), reused 1401 (delta 1401), pack-reused 2859 (from 2)\u001b[K\n",
            "Receiving objects: 100% (4393/4393), 650.71 MiB | 27.57 MiB/s, done.\n",
            "Resolving deltas: 100% (2659/2659), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "O10_T_xSKJlf",
        "outputId": "8dbbe598-3dfb-46b8-89f6-f67b95fc99c6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get data"
      ],
      "metadata": {
        "id": "nrzg3TaSKLAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove .zip file\n",
        "    os.remove(data_path / \"pizza_steak_sushi.zip\")\n",
        "\n",
        "# Setup Dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lt_CNQ4rKPmg",
        "outputId": "0f320e7d-b734-443b-9b22-e7bfbce6aaad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare data"
      ],
      "metadata": {
        "id": "PGaMWWaoKQlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "VNIQNEQVKVXu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoader's as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=simple_transform, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                               batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njd5lHTcKW23",
        "outputId": "5e410bbd-fd6a-4882-f0a4-e98b03408769"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7e5521632fd0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7e5621e9cc10>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get and prepare a pretrained model"
      ],
      "metadata": {
        "id": "Ciw2DiRHKaSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the model with pretrained weights and send it to the target device\n",
        "model_0 = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n",
        "#model_0 # uncomment to output (it's very long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "snUuRXd8Kdk5",
        "outputId": "7c86a4f8-4b80-4acd-e640-cd5a91e9e5c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b0_rwightman-7f5810bc.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b0_rwightman-7f5810bc.pth\n",
            "100%|██████████| 20.5M/20.5M [00:00<00:00, 50.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model_0.features.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "id": "IbRhGvy_KeVL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Get the length of class_names (one output unit for each class)\n",
        "output_shape = len(class_names)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model_0.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(in_features=1280,\n",
        "                    out_features=output_shape, # same number of output units as our number of classes\n",
        "                    bias=True)).to(device)"
      ],
      "metadata": {
        "id": "G1-6xV3ZKeSX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model"
      ],
      "metadata": {
        "id": "XQFaXX8CKePi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_0.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "exxU79eaKeM6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "model_0_results = engine.train(model=model_0,\n",
        "                       train_dataloader=train_dataloader,\n",
        "                       test_dataloader=test_dataloader,\n",
        "                       optimizer=optimizer,\n",
        "                       loss_fn=loss_fn,\n",
        "                       epochs=5,\n",
        "                       device=device)\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "57241e3ef94a42879becdbf83e300570",
            "53cdb27fecaf4e0fbfca1477b46846ca",
            "32e4b2b439c94e1cbb3fb5227e6a0e8e",
            "9ed5232a64dd415db8c482296c340efe",
            "d1c55914b6a7415caffedfb0fe3ee017",
            "45850c5b1c2843cf8ba0217f5a0c8e3d",
            "1a10a63d287c4656972c407d8f251ab2",
            "7df354546b71490d8fc3890d49ed564a",
            "45eda459e5584bc599d35a655a814519",
            "b1532d532ccc4df6991d37f6375a7ce8",
            "0162e6980bb84270a640f9c7bc0e4b1f"
          ]
        },
        "id": "ComVkVtuKeKG",
        "outputId": "0bd9ac3a-7e96-48f0-d362-2888a30991b3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57241e3ef94a42879becdbf83e300570"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 1.0895 | train_acc: 0.4414 | test_loss: 0.9202 | test_acc: 0.5085\n",
            "Epoch: 2 | train_loss: 0.8682 | train_acc: 0.7734 | test_loss: 0.8022 | test_acc: 0.7434\n",
            "Epoch: 3 | train_loss: 0.7771 | train_acc: 0.7812 | test_loss: 0.7399 | test_acc: 0.7737\n",
            "Epoch: 4 | train_loss: 0.7249 | train_acc: 0.7422 | test_loss: 0.6472 | test_acc: 0.8864\n",
            "Epoch: 5 | train_loss: 0.6445 | train_acc: 0.7812 | test_loss: 0.6244 | test_acc: 0.8968\n",
            "[INFO] Total training time: 22.691 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make predictions on the entire test dataset with the model"
      ],
      "metadata": {
        "id": "xFS4lE_IKyE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "\n",
        "model_0.eval()\n",
        "\n",
        "test_labels = []\n",
        "with torch.inference_mode():\n",
        "  for batch, (X, y) in enumerate(test_dataloader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    logits = model_0(X)\n",
        "    pred_label = torch.argmax(torch.softmax(logits, dim=1), dim=1)\n",
        "    test_labels.append(pred_label)\n",
        "test_labels = torch.cat(test_labels).cpu()\n",
        "test_labels\n"
      ],
      "metadata": {
        "id": "DwZuCluFu375",
        "outputId": "78f2a9ee-d3d0-4fd1-cdc9-e68f7d07a7b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 2, 1, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2,\n",
              "        2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2,\n",
              "        2, 2, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Make a confusion matrix with the test preds and the truth labels"
      ],
      "metadata": {
        "id": "Mb2bQ1b5K2WP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need the following libraries to make a confusion matrix:\n",
        "* torchmetrics - https://torchmetrics.readthedocs.io/en/stable/\n",
        "* mlxtend - http://rasbt.github.io/mlxtend/"
      ],
      "metadata": {
        "id": "5I2jpYAcM07s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# See if torchmetrics exists, if not, install it\n",
        "try:\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")\n",
        "    assert int(mlxtend.__version__.split(\".\")[1]) >= 19, \"mlxtend verison should be 0.19.0 or higher\"\n",
        "except:\n",
        "    !pip install -q torchmetrics -U mlxtend # <- Note: If you're using Google Colab, this may require restarting the runtime\n",
        "    import torchmetrics, mlxtend\n",
        "    print(f\"mlxtend version: {mlxtend.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcKYZGWuK2P8",
        "outputId": "77d91227-7a55-45b5-af12-06ae051b76f7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m884.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hmlxtend version: 0.23.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import mlxtend upgraded version\n",
        "import mlxtend\n",
        "print(mlxtend.__version__)\n",
        "assert int(mlxtend.__version__.split(\".\")[1]) >= 19 # should be version 0.19.0 or higher"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOYVew4xMxgI",
        "outputId": "e67c9617-e57a-48b1-f86a-78f4dc1c4dd9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.23.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO\n",
        "from torchmetrics import ConfusionMatrix\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "\n",
        "test_truth = torch.cat([y for X, y in test_dataloader])\n",
        "\n",
        "confmat = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_names))\n",
        "confmat_tensor = confmat(preds=test_labels, target=test_truth)\n",
        "\n",
        "fig, ax = plot_confusion_matrix(confmat_tensor.numpy(), class_names=class_names, figsize=(10,7))\n"
      ],
      "metadata": {
        "id": "_5LU9-5Xu7dP",
        "outputId": "a54b4063-02ea-4c87-ca18-91adf1b1638c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Get the \"most wrong\" of the predictions on the test dataset and plot the 5 \"most wrong\" images. You can do this by:\n",
        "* Predicting across all of the test dataset, storing the labels and predicted probabilities.\n",
        "* Sort the predictions by *wrong prediction* and then *descending predicted probabilities*, this will give you the wrong predictions with the *highest* prediction probabilities, in other words, the \"most wrong\".\n",
        "* Plot the top 5 \"most wrong\" images, why do you think the model got these wrong?\n",
        "\n",
        "You'll want to:\n",
        "* Create a DataFrame with sample, label, prediction, pred prob\n",
        "* Sort DataFrame by correct (does label == prediction)\n",
        "* Sort DataFrame by pred prob (descending)\n",
        "* Plot the top 5 \"most wrong\" image predictions"
      ],
      "metadata": {
        "id": "YqlStPo-gbrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "cHtMeYHuvDwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Predict on your own image of pizza/steak/sushi - how does the model go? What happens if you predict on an image that isn't pizza/steak/sushi?\n",
        "* Here you can get an image from a website like http://www.unsplash.com to try it out or you can upload your own."
      ],
      "metadata": {
        "id": "1IvuTskxgjaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get an image of pizza/steak/sushi\n"
      ],
      "metadata": {
        "id": "C16glgVFglmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Get an image of not pizza/steak/sushi\n"
      ],
      "metadata": {
        "id": "clA_KmihVYyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Train the model from section 4  in notebook 06 part 3 for longer (10 epochs should do), what happens to the performance?\n",
        "\n",
        "* See the model in notebook 06 part 3 for reference: https://www.learnpytorch.io/06_pytorch_transfer_learning/#3-getting-a-pretrained-model"
      ],
      "metadata": {
        "id": "Vzvi8GprgmJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Recreate a new model\n"
      ],
      "metadata": {
        "id": "kIKg53Jna-Rt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Train the model for 10 epochs"
      ],
      "metadata": {
        "id": "JhGT9igPgoF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Train the model from section 4 above with more data, say 20% of the images from Food101 of Pizza, Steak and Sushi images.\n",
        "* You can find the [20% Pizza, Steak, Sushi dataset](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi_20_percent.zip) on the course GitHub. It was created with the notebook [`extras/04_custom_data_creation.ipynb`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/04_custom_data_creation.ipynb).\n"
      ],
      "metadata": {
        "id": "_oRrWPZTgoqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get 20% data"
      ],
      "metadata": {
        "id": "VxyMMnUbgvw2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi_20_percent\"\n",
        "image_data_zip_path = \"pizza_steak_sushi_20_percent.zip\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / image_data_zip_path, \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / image_data_zip_path, \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi 20% data...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove .zip file\n",
        "    os.remove(data_path / image_data_zip_path)\n",
        "\n",
        "# Setup Dirs\n",
        "train_dir_20_percent = image_path / \"train\"\n",
        "test_dir_20_percent = image_path / \"test\"\n",
        "\n",
        "train_dir_20_percent, test_dir_20_percent"
      ],
      "metadata": {
        "id": "U_fdu5m2eKT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create DataLoaders"
      ],
      "metadata": {
        "id": "SQj7eFdSe4Fv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a transforms pipeline\n",
        "simple_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])"
      ],
      "metadata": {
        "id": "TEG_k785e7Jw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create training and testing DataLoader's as well as get a list of class names\n",
        "train_dataloader_20_percent, test_dataloader_20_percent, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n",
        "                                                                                                     test_dir=test_dir_20_percent,\n",
        "                                                                                                     transform=simple_transform, # resize, convert images to between 0 & 1 and normalize them\n",
        "                                                                                                     batch_size=32) # set mini-batch size to 32\n",
        "\n",
        "train_dataloader_20_percent, test_dataloader_20_percent, class_names"
      ],
      "metadata": {
        "id": "82x7LnQJe7H5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get a pretrained model"
      ],
      "metadata": {
        "id": "qROl77sKfIOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "PHWNZ6yDvpR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train a model with 20% of the data"
      ],
      "metadata": {
        "id": "UqffJfOIfp3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "wXpYOYeTvp7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Try a different model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html) on the Pizza, Steak, Sushi data, how does this model perform?\n",
        "* You'll have to change the size of the classifier layer to suit our problem.\n",
        "* You may want to try an EfficientNet with a higher number than our B0, perhaps `torchvision.models.efficientnet_b2()`?\n",
        "  * **Note:** Depending on the model you use you will have to prepare/transform the data in a certain way."
      ],
      "metadata": {
        "id": "Ibj4UPjRgvly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO"
      ],
      "metadata": {
        "id": "3FQ8tL7El7eO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}